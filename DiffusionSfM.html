<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="DiffusionSfM: Predicting Structure and Motion via Ray Origin and Endpoint Diffusion">
  <meta name="keywords" content="Sparse view, reconstruction, diffusion, Structure-from-Motion">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DiffusionSfM: Predicting Structure and Motion via Ray Origin and Endpoint Diffusion</title>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-YXDZHQRVSJ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-YXDZHQRVSJ');
</script>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h2 class="title is-2 publication-title">DiffusionSfM: Predicting Structure and Motion via Ray Origin and Endpoint Diffusion</h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://qitaozhao.github.io/" target="_blank">Qitao Zhao</a>,</span>
            <span class="author-block">
              <a href="https://amyxlase.github.io/" target="_blank">Amy Lin</a>,</span>
            <span class="author-block">
              <a href="https://jefftan969.github.io/" target="_blank">Jeff Tan</a>,</span>
            <span class="author-block">
              <a href="https://jasonyzhang.com/" target="_blank">Jason Y. Zhang</a>,</span>
            <span class="author-block">
              <a href="https://www.cs.cmu.edu/~deva/" target="_blank">Deva Ramanan</a>,</span>
            <span class="author-block">
              <a href="https://shubhtuls.github.io/" target="_blank">Shubham Tulsiani</a></span>
          </div>

          <!-- <div class="is-size-5 publication-authors">
            <span class="author-block">Carnegie Mellon University</span>
          </div> -->

          <!-- <div class="is-size-5 publication-authors">
            <span class="author-block">Submitted to CVPR 2025</span>
          </div> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="research/DiffusionSfM/DiffusionSfM.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv (coming soon)</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="https://qitaozhao.github.io/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon)</span>
                  </a>
              </span> -->
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
              <!-- <span class="link-block">
                <a href="diverse.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <span>Random Results</span>
                  </a>
              </span> -->

              <!-- <span class="link-block">
                <a href="comparison.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <span>Random Comparisons</span>
                  </a>
              </span> -->

            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./research/DiffusionSfM/figures/teaser_new.png" alt="Teaser figure."/>
      <h5 class="">
        Given a set of multi-view images as input, <strong>DiffusionSfM</strong> parametrizes scene geometry and cameras as pixel-wise ray origins and endpoints in a global frame and learns a denoising diffusion model to infer these from multi-view input. In contrast to current Structure-from-Motion pipelines, which often adopt a two-stage approach of pairwise reasoning followed by global optimization, our method unifies both stages into a single end-to-end multi-view reasoning step.
      </h5>
    </div>
  </div>
</section>


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-one">
          <video poster="" id="one" autoplay muted loop playsinline height="100%">
            <source src="static/images/teaser1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-two">
          <video poster="" id="two" autoplay muted loop playsinline height="100%">
            <source src="static/images/teaser2.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-three">
          <video poster="" id="three" autoplay muted loop playsinline height="100%">
            <source src="static/images/teaser3.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-four">
          <video poster="" id="four" autoplay muted loop playsinline height="100%">
            <source src="static/images/teaser4.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-five">
          <video poster="" id="five" autoplay muted loop playsinline height="100%">
            <source src="static/images/teaser5.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-six">
          <video poster="" id="six" autoplay muted loop playsinline height="100%">
            <source src="static/images/teaser6.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Current Structure-from-Motion (SfM) methods often adopt a two-stage pipeline involving learned or geometric pairwise reasoning followed by a global optimization. We instead propose a data-driven multi-view reasoning approach that directly infers cameras and 3D geometry from multi-view images. Our proposed framework, <em>DiffusionSfM</em>, parametrizes scene geometry and cameras as pixel-wise ray origins and endpoints in a global frame, and learns a transformer-based denoising diffusion model to predict these from multi-view input. We develop mechanisms to overcome practical challenges in training diffusion models with missing data and unbounded scene coordinates, and demonstrate that DiffusionSfM allows accurate prediction of 3D and cameras. We empirically validate our approach on challenging real world data and find that DiffusionSfM improves over prior classical and learning-based methods, while also naturally modeling uncertainty and allowing external guidance to be incorporated in inference. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/5077xFjWsIs"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Method</h2>

        <!-- Subsection. -->
        <!-- <h3 class="title is-4">Comparison Against Existing Methods</h3> -->
        <div class="content has-text-justified">
          <p>
            Given sparse multi-view images as input, DiffusionSfM predicts pixel-wise ray origins and endpoints for each image in a global frame using a denoising diffusion process. During training, it is conditioned on a depth mask to handle missing or invalid ground truth depth common in real-world datasets, e.g., CO3D. At inference, the depth mask is set to all ones, enabling the model to predict origins and endpoints for all pixels.
          </p>
        </div>
        <div class="content has-text-centered">
          <img src="./research/DiffusionSfM/figures/framework.png" alt="Method figure."/>
        </div>
        <!--/ Subsection. -->
      </div>
    </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <!-- <div class="column">
        <div class="content">
          <h2 class="title is-3">Left</h2>
          <p>
            p
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="#"
                    type="video/mp4">
          </video>
        </div>
      </div> -->
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <!-- <div class="column">
        <h2 class="title is-3">Right</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              p
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src=""
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div> -->
    <!--/ Matting. -->


    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Results</h2>

        <!-- Subsection. -->
        <h3 class="title is-4">Quantitative Comparison of Camera Pose Accuracy on CO3D</h3>
        <div class="content has-text-justified">
          <p>
            On the left, we report the proportion of relative camera rotations within 15° of the ground truth. On the right, we report the proportion of camera centers within 10% of the scene scale. To align the predicted camera centers to ground-truth, we apply an optimal similarity transform, hence the alignment is perfect at N=2 but worsens with more images. DiffusionSfM outperforms all other methods for camera center accuracy, and outperforms all methods trained on equivalent data for rotation accuracy.
          </p>
        </div>
        <div class="content has-text-centered">
          <img src="./research/DiffusionSfM/figures/pose_acc.png" alt="SPARF Comparison figure."/>
        </div>
        <!--/ Subsection. -->

        <!-- Subsection. -->
        <h3 class="title is-4">Quantitative Comparison of Predicted Geometry and Focal Length on CO3D Unseen Categories</h3>
        <div class="content has-text-justified">
          <p>
            Top: Chamfer Distance (CD) computed over all scene points. Middle: CD computed on foreground points only. Bottom: Percentage of predicted focal lengths within 15% of the ground truth. RD+MoGe refers to the Ray Diffusion camera pose, with depth estimates from MoGe aligned to the ground truth. DUSt3R-CO3D is trained solely on CO3D, while DUSt3R-all is trained on multiple datasets. DiffusionSfM outperforms all methods in terms of full scene geometry and estimated focal length, and also outperforms both RD+MoGe and DUSt3R-CO3D on foreground geometry.
          </p>
        </div>
        <div class="content has-text-centered">
            <div class="content has-text-centered">
          <img src="./research/DiffusionSfM/figures/geometry_acc.png" alt="LEAP Comparison figure." width="75%"/>
        </div>
          <!-- <video class="rounded" id="replay-video"
                 controls
                 autoplay
                 loop
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="static/images/comp_v10_core.mp4"
                    type="video/mp4">
          </video> -->
        </div>
        <!--/ Subsection. -->

        <!-- Subsection. -->
        <h3 class="title is-4">Qualitative Comparison of Predicted Geometry and Camera Poses</h3>
        <div class="content has-text-justified">
          <p>
            DiffusionSfM shows superior capabilities in handling challenging samples, e.g., the skateboard and tennis ball. Additionally, while we observe that DUSt3R-all can predict highly precise camera rotations, it often struggles with camera centers (see the keyboard and backpack examples).
          </p>
        </div>
        <div class="content has-text-centered">
            <div class="content has-text-centered">
          <img src="./research/DiffusionSfM/figures/supp_vis_compare_new.png" alt="LEAP Comparison figure." width="100%"/>
        </div></div>
        <!--/ Subsection. -->

        <!-- Subsection. -->
        <div class="content has-text-centered">
            <div class="content has-text-centered">
          <img src="./research/DiffusionSfM/figures/supp_vis_cameras.png" alt="LEAP Comparison figure." width="90%"/>
        </div>
        </div>
        <!--/ Subsection. -->

        <!-- Subsection. -->
        <h3 class="title is-4">Visualizations of the Effect of Mono-Depth Diffusion Guidance</h3>
        <div class="content has-text-justified">
          <p>
            We utilize mono-depth estimates from MeGe to guide the x_0-prediction from our model towards more accurate, clean estimates. This guidance enhances the quality of the predicted geometry while preserving multi-view consistency.
          </p>
        </div>
        <div class="content has-text-justified">
          <!-- <p>
            Please see randomly selected results from all 51 categories <a href="diverse.html">here</a>. 
          </p> -->
        </div>
        <div class="content has-text-centered">
            <div class="content has-text-centered">
          <img src="./research/DiffusionSfM/figures/supp_vis_diffusion_guidance.png" alt="LEAP Comparison figure." width="100%"/>
        </div>
        <!--/ Subsection. -->
        </div>

      </div>
    </div>
    <!--/ Animation. -->


    <!-- Concurrent Work. -->
    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
        </div>
      </div>
    </div>
    </div> -->

</section>




<!-- <section class="section" id="Acknowledgements">
  <div class="container is-max-desktop content">
    <h3 class="title">Related Links</h2>
    <p>Our 3D reconstruction pipeline is built on top of <a href="https://arxiv.org/pdf/2309.16653" target="_blank">DreamGaussian</a> and <a href="https://github.com/ashawkey/stable-dreamfusion" target="_blank">Stable-DreamFusion</a>. Additionally, we fine-tuned and utilized the novel-view generative priors from <a href="https://arxiv.org/pdf/2303.11328" target="_blank">Zero123</a>. We sincerely appreciate the authors' efforts in open-sourcing their code. </p>
</section> -->

<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h3 class="title">BibTeX</h3>
    <pre>
<code>@inproceedings{zhao2024sparseags,
  title={Sparse-view Pose Estimation and Reconstruction via Analysis by Generative Synthesis}, 
  author={Qitao Zhao and Shubham Tulsiani},
  booktitle={NeurIPS},
  year={2024}
}</code></pre>
  </div>
</section> -->

<!-- <section class="column" id="Acknowledgements">
  <div class="container is-max-desktop content">
    <h3 class="title">Acknowledgements</h2>
    <p>We thank Zihan Wang and the members of the Physical Perception Lab at CMU for their valuable discussions. We especially appreciate Amy Lin and Zhizhuo (Z) Zhou for their assistance in creating figures, as well as Yanbo Xu and Jason Zhang for their feedback on the draft. We also thank Hanwen Jiang for his support in setting up the LEAP baseline for comparison.</p>

    <p>This work was supported in part by NSF Award IIS-2345610. This work used Bridges-2 at Pittsburgh Supercomputing Center through allocation CIS240166 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296.
    </p>
  </div>
</section> -->

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/abs/2212.00792">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/zhizdev/sparsefusion" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is made from this template <a href="https://github.com/nerfies/nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
